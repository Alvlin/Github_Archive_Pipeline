{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3794b28-f144-46d0-9a5a-fc1d4b78a72c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cleaning and Transformation Steps\n",
    "\n",
    "## Actor Table\n",
    "- Import nested column actor.id renamed as actor_id\n",
    "- Drop duplicates on import based on values in 'actor_id' column\n",
    "- Drop Null values based on values in 'actor_id' column\n",
    "- Add column 'is_bot' that determines whether actor is a bot by checking if the login contains the string '[bot]'\n",
    "- Enforce schema to correct data types and nullability\n",
    "\n",
    "## Org Table\n",
    "- Drop duplicates on import based on 'org.id' column.\n",
    "- Select nested fields from all non-null orgs \n",
    "- Drop null orgs based on 'org_id' column\n",
    "- Enforce schema to correct data types and nullability\n",
    "\n",
    "## Repo Table\n",
    "- Drop duplicates on import based on 'repo.id' column.\n",
    "- Enforce schema to correct data types and nullability\n",
    "\n",
    "## Event Table\n",
    "- Enforce schema to correct data types and nullability.\n",
    "\n",
    "## PushEvent Table\n",
    "- On import:\n",
    "  - Drop any records where the commit_id is null\n",
    "  - Add column 'is_main' that determines whether the event affects the main branch using the branch_ref column.\n",
    "\n",
    "## Commit Table\n",
    "- On import, drop any records where the 'commit_id' is null.\n",
    "- Use raw text from commit_message column to determine the language of the comment using the sparknlp library then store the language in a new column named language\n",
    "- Enforce schema to correct data types and nullability.\n",
    "\n",
    "# Partitioning Strategy\n",
    "\n",
    "In order to determine the appropriate number of partitions, we followed the following steps:\n",
    "- Import a sample size of two days of data.\n",
    "- Transform these two days into the appropriate tables and export those tables as parquet files.\n",
    "- Use the size of these files to estimate the size of the table once it contains the full month of data\n",
    "  - Divide size by two to get average size of daily data.\n",
    "  - Multiply by 31 to estimate the size of a month a data\n",
    "  - Divide that size estimate for a month of date by 128mb to determine the approximate number of partitions needed\n",
    "  - This last step was repeated for each individual table in our erd."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Documentation for Bronze --> Silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
